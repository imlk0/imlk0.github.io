<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>[arxiv cs.DC]Safe and Practical GPU Acceleration in TrustZone 论文阅读 | imlk's blog</title>
<link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;700&display=swap" rel=stylesheet></head><body><header class=header><a class=logo href=/>imlk's Blog</a><nav><ul class=menu><li><a href=/>🏠Home</a></li><li><a href=/about/>👋About</a></li><li><a href=/archives/>📜Archives</a></li><li><a href=/friends/>🔗Friends</a></li><li><a href=/dn42/>🕸️DN42</a></li><li><a href=/index.xml>📢Subscribe</a></li></ul></nav></header><hr><div class=article-meta><h1><span class=title>[arxiv cs.DC]Safe and Practical GPU Acceleration in TrustZone 论文阅读</span></h1><span><span class=date>📅 2021-12-02</span>
<span class=date>(更新于2023-08-30)</span>
/
<span class=cats>📚
<a href=https://blog.imlk.top/categories/tee/>TEE</a>
</span>/
<span class=tags>🏷️
<a href=https://blog.imlk.top/tags/trustzone/>#TrustZone</a>
<a href=https://blog.imlk.top/tags/security/>#Security</a>
<a href=https://blog.imlk.top/tags/gpu/>#GPU</a>
<a href=https://blog.imlk.top/tags/reading/>#Reading</a></span></span><br></div><main class=post-content><h1 id=links>Links</h1><ul><li>arxiv cs.DC 2021-11-04 <a href=https://arxiv.org/pdf/2111.03065.pdf>https://arxiv.org/pdf/2111.03065.pdf</a></li></ul><h1 id=主要内容>主要内容</h1><h2 id=problem>Problem</h2><ul><li>GPU软件栈过大（数十MB）、部分代码依赖于POSIX层</li><li>移植GPU软件栈工程量大、使TCB膨胀、引入新的安全问题</li><li>将运算外包到普通世界的GPU软件栈，需要防止数据/模型参数被学习/结果篡改：同态加密、ML工作负载转换、结果验证。</li></ul><h2 id=key-idea>Key idea</h2><ul><li><p>提出一种名为协作试运行(collaborative dryrun, CODY)的方法，<strong>实现了一种记录CPU和GPU交互的环境</strong></p><p><img src=/images/%5Barxiv%20cs%20DC%5DSafe%20and%20Practical%20GPU%20Acceleration%20i%208447537175cb4080a8ad49849a97d472/Untitled.png alt=Untitled></p></li><li><p>记录阶段：</p><ul><li><p><strong>云服务运行GPU软件栈</strong>，而无任何GPU硬件</p></li><li><p>Client端<strong>TEE中的程序请求云服务</strong>运行一个负载(例如一个模型推断任务)</p></li><li><p>云服务中在没有实际GPU硬件的情况下执行GPU软件栈，将CPU和GPU之间的交互<strong>传递到Client端的TEE程序</strong>中（协作）</p></li><li><p>云服务<strong>记录对于该负载的所有的GPU交互</strong></p><p><img src=/images/%5Barxiv%20cs%20DC%5DSafe%20and%20Practical%20GPU%20Acceleration%20i%208447537175cb4080a8ad49849a97d472/Untitled%201.png alt=Untitled></p></li></ul></li><li><p>重放阶段：</p><ul><li>对于<strong>新的输入</strong>，TEE在没有云服务交互的情况下，在物理GPU上重放之前的记录</li></ul></li><li><p>优点：</p><ul><li>安全、可管理的记录环境，云服务运行在严格管理的基础设施上运行，与客户端TEE之间执行经过验证的加密通信</li><li>云服务不会知道TEE的敏感数据：<strong>模型输入</strong>、<strong>模型参数</strong></li><li>云服务<strong>可以访问不同的GPU硬件而无需托管它们</strong>，而是负责托管GPU硬件的驱动程序</li></ul></li></ul><h2 id=challenges-and-designs>Challenges and Designs</h2><h3 id=challenges>Challenges</h3><ul><li>跨越Cloud和Client之间的连接的CPU/GPU之间的交互。<strong>时延问题</strong><ul><li>registerr accesses、accesses to shared memory、interrupts</li></ul></li></ul><h3 id=designs>Designs</h3><p>提出两个见解：</p><ul><li>GPU的寄存器访问序列包含许多重复的段：可以<strong>预测寄存器访问及结果</strong></li><li>Cloud<strong>只需要产生可重放的交互序列</strong>，不需要保证正确性</li></ul><p>设计上，CODY会自动化地插桩GPU驱动代码，以实现：</p><ul><li>寄存器访问的推迟：将多个寄存器访问请求批量提交给Client；将<strong>未提交的寄存器读请求表示为符号(symbol)</strong>，允许<strong>驱动程序的符号执行</strong>(symbolic execution of the driver)，在Client返回结果后使用具体的寄存器值替换符号变量（symbolic variables）</li><li>寄存器访问的预测：预测寄存器读的结果，并允许驱动用预测的结果继续执行；在Client返回结果后验证预测是否正确；如果预测失败，cloud和client端都使用GPU重放技术快速回滚到他们最近的有效状态</li><li>仅metastate(元状态？)的同步（Metastate-only synchronization）：cloud和client必须保持内存同步；通过挖掘GPU硬件事件减少同步频率；通过仅同步GPU的metastate（如GPU shaders, command lists, and job descriptions），省略构成GPU内存的大部分工作负载数据；确保交互的正确性，放弃计算结果的正确性</li></ul><h2 id=environment>Environment</h2><ul><li>Cloud：Odroid C4，Arm board，<ul><li>GPU栈：a ML framework (ACL v20.05), a runtime (libmali.so), a driver (Mali Bifrost r24)</li></ul></li><li>Client：Hikey960 with Mali G71 MP8 GPU<ul><li>Debian 9.13(Linux v4.19), OPTEE (v3.12)</li></ul></li><li>使用多种不同的ML负载</li></ul><h2 id=威胁模型>威胁模型</h2><ul><li>信任cloud，信任其上的GPU堆栈，信任client端的TEE</li><li>威胁：客户端TEE之外部分的威胁、来自网络通信窃听的威胁</li></ul><h2 id=实现>实现</h2><ul><li>代码插桩工具：实现为一个Clang plugin，静态分析和代码修改，分析驱动程序的抽象语法树</li><li>DriverShim：1K SLoC，内核模块，由被插桩后的驱动程序调用、执行<strong>依赖跟踪</strong>、提交的管理、预测</li><li>GPUShim</li></ul><h1 id=details>Details</h1><h2 id=cpugpu之间的交互类型>CPU/GPU之间的交互类型</h2><ul><li>寄存器</li><li>共享内存（包括GPU专用的页表，可以映射到GPU内存或CPU内存）</li><li>来自GPU的中断</li></ul><h2 id=重放流程>重放流程</h2><ul><li>TEE中内置一个简单的replayer(30KB)</li><li>可以选择创建一整个recording，也可以选择一次记录中为模型的每个层分别创建recordings：</li></ul><p><img src=/images/%5Barxiv%20cs%20DC%5DSafe%20and%20Practical%20GPU%20Acceleration%20i%208447537175cb4080a8ad49849a97d472/Untitled%202.png alt=Untitled></p><h2 id=优化技术>优化技术</h2><h3 id=推迟寄存器访问5b>推迟寄存器访问（5b）</h3><p><img src=/images/%5Barxiv%20cs%20DC%5DSafe%20and%20Practical%20GPU%20Acceleration%20i%208447537175cb4080a8ad49849a97d472/Untitled%203.png alt=Untitled></p><ul><li>持续执行，直至驱动无法在没有真实的值的情况下继续执行下去，驱动暂停</li><li>此时cloud端异步地提交(commit)所有被推迟访问的的请求</li><li>实现：通过自动化工具向驱动注入hook函数</li><li>正确性：<ul><li>访问的正确性<ul><li>client端寄存器<strong>访问顺序需保持一致</strong></li><li>处理寄存器访问触发的<strong>隐藏依赖</strong>，例如读中断寄存器可能自动清空中断状态</li><li>处理：每个内核线程对应一个队列</li></ul></li><li>数据依赖&控制依赖<ul><li>读不存在、读后写、读后分支</li><li><strong>用符号(symbol)替代读的结果</strong>，并传播到后续的使用中，在提交返回时将这些符号替换为具体值</li></ul></li></ul></li><li>提交时机<ul><li>分支逻辑依赖于读寄存器值</li><li>调用内核api，尤其是scheduling、locking等</li><li>驱动明确地调用了延迟函数</li></ul></li><li>如何保证多线程的一致性？<ul><li>假定驱动程序用锁来更新共享变量</li><li>在释放锁之前提交</li></ul></li></ul><h3 id=预测5c>预测(5c)</h3><ul><li>提交后不阻塞等待、而是继续执行</li><li>用预测的提交C中的寄存值来继续执行；当提交返回后再进行验证，如果预测失败则进行恢复。不影响正确性</li><li>保守预测：只在该位置<strong>最近的k次提交都给出一样的返回</strong>时，才运行使用预测的结果，实验中k=3</li><li>正确性：<ul><li>新提交在已有提交返回并验证后发出</li><li>发生内核状态外化时（如printk），阻塞执行：通过拦截十几个外化API来实现（可能不够全面）</li><li>跟踪<strong>预测值寄存器的访问：寄存器染色、污点跟踪</strong></li></ul></li><li>预测失败的恢复<ul><li>CPU/GPU状态都要恢复</li><li>cloud向client发送发生错误预测的位置，双方<strong>使用记录的日志重放</strong>，快速<strong>重新启动</strong>，期间不需要通信</li></ul></li></ul><h3 id=卸载轮询循环offloading-polling-loops>卸载轮询循环（Offloading polling loops）</h3><ul><li>将一些循环polling的情况一次性卸载到client端</li></ul><h3 id=选择性内存同步>选择性内存同步</h3><ul><li>问题：CPU和GPU的内存共享协议方法未被明确定义，不使用锁</li><li>思路：将GPU任务数量限制为1，使CPU和GPU串行执行，不会同时访问共享内存</li><li>时机：<ul><li>cloud→client：启动新的GPU作业之前</li><li>client→cloud：客户端发出作业完成的中断</li></ul></li></ul><h2 id=实验>实验</h2><h3 id=性能部分>性能部分</h3><p>四种策略：Naive、OursM、OursMD、OursMDS（完整的CODY）</p><p>两种场景：Wifi、蜂窝网络</p><p>六种模型：MNIST、Alex、Mobile、Squeeze、Res12、VGG1</p><ul><li>比较Recording delays<ul><li>Naive+wifi: 52秒-423秒</li><li>OursMDS：用时平均减少95%，18秒（Wifi），30秒（蜂窝网络）</li></ul></li><li>比较Replay delays<ul><li><p>使用CODY重放，比直接在普通世界运行GPU堆栈减少了3%-68%的时间</p><p><img src=/images/%5Barxiv%20cs%20DC%5DSafe%20and%20Practical%20GPU%20Acceleration%20i%208447537175cb4080a8ad49849a97d472/Untitled%204.png alt=Untitled></p></li></ul></li></ul></main><div id=comments><hr><script src=//geoip-js.com/js/apis/geoip2/v2.1/geoip2.js type=text/javascript></script><script>var onSuccess=function(e){var t=e.country.names["zh-CN"],n=e.country.iso_code;if(!n)return;t&&(document.getElementById("comments-blocked-country-code").innerHTML=t),n=="CN"&&(document.getElementById("comments-shown").style.display="none",document.getElementById("comments-blocked").style.display="")},onError=function(){};geoip2.country(onSuccess,onError)</script><div id=comments-blocked style=font-style:italic;color:#777;display:none>评论区已关闭。<span style=float:right>来自 <span id=comments-blocked-country-code>unknown</span> 的访客
<span>&nbsp; - powered by: geoip-js.com</span></span></div><div id=comments-shown><script src=https://utteranc.es/client.js repo=KB5201314/KB5201314.github.io issue-term=pathname label='💬 comments 💬' theme=github-light crossorigin=anonymous async></script></div></div><footer class=footer><hr>© imlk 2017 &ndash; 2023 | <a href=https://blog.imlk.top>imlk's blog</a> | <a href=https://github.com/KB5201314/>Github</a> | <a href=mailto:me@imlk.top>Email</a> | <a href=https://beian.miit.gov.cn/>京ICP备2020042968号-1</a></footer></body></html>